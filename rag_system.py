# -*- coding: utf-8 -*-
"""RAG system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1khMz2WspV_jFqeVn3jVZr2qdVdxOVlHR
"""

import fitz
import faiss
import numpy as np
import torch
import re
import nltk
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from docx import Document
from sklearn.metrics.pairwise import cosine_similarity


nltk.download('punkt')


def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()

def clean_output(text):
    sentences = sent_tokenize(text.strip())
    if not sentences:
        return ""

    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    sentence_embeddings = embedder.encode(sentences)

    unique_sentences = []
    seen_embeddings = []

    for idx, sentence in enumerate(sentences):
        current_vec = sentence_embeddings[idx].reshape(1, -1)

        if seen_embeddings:
            sims = cosine_similarity(current_vec, np.vstack(seen_embeddings))[0]
            if np.max(sims) >= 0.7:
                continue

        unique_sentences.append(sentence.strip())
        seen_embeddings.append(current_vec)

    return " ".join(unique_sentences).strip()



def write_answer_to_docx(answer, output_path="Task1_Output.docx"):
    doc = Document()
    doc.add_heading("Task 1 Output: Response", level=1)
    doc.add_paragraph(answer)
    doc.save(output_path)
    print(f" Saved final response to {output_path}")


def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])


def split_text(text, chunk_size=500, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)


def create_vector_index(chunks, embedder):
    vectors = embedder.encode(chunks)
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(np.array(vectors))
    return index, vectors


def retrieve_chunks(query, embedder, index, chunks, k=5):
    query = query.lower().strip()
    qvec = embedder.encode([query])
    _, idx = index.search(np.array(qvec), k)
    return [chunks[i] for i in idx[0]]


def generate_answer(query, context, model, tokenizer):
    prompt = f"""[INST] <<SYS>>
You are a helpful assistant. Use the context to answer the question below.
<</SYS>>

Context:
{context}

Question: {query}
Answer:[/INST]"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    pdf_path = "Chandrayaan-3 Mission.pdf"
    query = "What does the Diviner instrument measure on the Moon?"

    print(" Extracting text from PDF.")
    raw_text = extract_text_from_pdf(pdf_path)

    print(" Cleaning text.")
    cleaned_text = clean_text(raw_text)
    print(f" Total characters in cleaned text: {len(cleaned_text)}")

    print(" Splitting into chunks.")
    chunks = split_text(cleaned_text)
    print(f" Total chunks created: {len(chunks)}")

    print(" Creating FAISS index.")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    index, _ = create_vector_index(chunks, embedder)

    print(" Retrieving top chunks for query.")
    top_chunks = retrieve_chunks(query, embedder, index, chunks)

    print(" Loading TinyLlama model (CPU).")
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model = model.to("cpu")

    print(" Generating answer.")
    context = "\n".join(top_chunks)
    raw_answer = generate_answer(query, context, model, tokenizer)

    print(" Cleaning final answer.")
    cleaned_answer = clean_output(raw_answer)

    print("\n Final Answer:\n", cleaned_answer)

    print(" Saving final answer to DOCX.")
    write_answer_to_docx(cleaned_answer, output_path="Task1_Output.docx")

if __name__ == "__main__":
    main()

import fitz
import faiss
import numpy as np
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from docx import Document

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n+', '\n', text)
    return text.strip()


def clean_prompt_output(text):
    return re.sub(r"\[/?INST\]|<<SYS>>|<</SYS>>", "", text).strip()


def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])


def extract_text_from_word(word_path):
    doc = Document(word_path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])


def split_text(text, chunk_size=500, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)


def create_vector_index(chunks, embedder):
    vectors = embedder.encode(chunks)
    index = faiss.IndexFlatL2(vectors.shape[1])
    index.add(np.array(vectors))
    return index, vectors


def retrieve_chunks(query, embedder, index, chunks, k=5):
    query = query.lower().strip()
    qvec = embedder.encode([query])
    _, idx = index.search(np.array(qvec), k)
    return [chunks[i] for i in idx[0]]


def generate_answer(query, context, model, tokenizer):
    prompt = f"""[INST] <<SYS>>
You are a helpful assistant. Use the context to answer the question below.
<</SYS>>

Context:
{context}

Question: {query}
Answer:[/INST]"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def main():
    pdf_path = "Chandrayaan-3 Mission.pdf"
    word_path = "Task1_Output.docx"
    query = "What does the Diviner instrument measure on the Moon?"

    print(" Extracting text from PDF...")
    pdf_text = extract_text_from_pdf(pdf_path)

    print(" Extracting text from Word document...")
    word_text = extract_text_from_word(word_path)

    print(" Cleaning and merging both texts...")
    combined_text = clean_text(pdf_text + "\n" + word_text)
    print(f" Combined text length: {len(combined_text)} characters")

    print(" Splitting into chunks...")
    chunks = split_text(combined_text)
    print(f" Total chunks: {len(chunks)}")

    print(" Creating embeddings + FAISS index...")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    index, _ = create_vector_index(chunks, embedder)

    print(" Retrieving top chunks for query...")
    top_chunks = retrieve_chunks(query, embedder, index, chunks)

    print(" Loading local model (TinyLlama)...")
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model = model.to("cpu")

    print(" Generating final answer...")
    context = "\n".join(top_chunks)
    raw_answer = generate_answer(query, context, model, tokenizer)

    cleaned_answer = clean_prompt_output(raw_answer)

    print("\n Final Answer:\n", cleaned_answer)

if __name__ == "__main__":
    main()

